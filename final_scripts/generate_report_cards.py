import argparse
import numpy as np
import pandas as pd
import json
import os
from edfio import *
from muniverse.evaluation.report_card_routines import *
from muniverse.data_preparation.data2bids import *
from muniverse.evaluation.evaluate import *
from pathlib import Path
import glob

def list_files(root, extension):
    files = list(root.rglob(f'*{extension}'))
    return files

def get_recording_info(source_file_name):
    splitname = source_file_name.split('_')

    if splitname[1].split('-')[0] == 'ses':
        sub = int(splitname[0].split('-')[1])
        ses = int(splitname[1].split('-')[1])
        task = splitname[2].split('-')[1]
        run = int(splitname[3].split('-')[1])
        data_type = splitname[4].split('.')[0]
    else:        
        sub = int(splitname[0].split('-')[1])
        task = splitname[1].split('-')[1]
        run = int(splitname[2].split('-')[1])
        ses = -1
        data_type = splitname[3].split('.')[0]

    return sub, ses, task, run, data_type

def main():
    parser = argparse.ArgumentParser(description='Convert the output of a decomposition in BIDS format')
    parser.add_argument('-r', '--bids_root',  default='/Users/thomi/Documents/muniverse-data', help='Path to the muniverse datasets')
    parser.add_argument('-a', '--algorithm', choices=['scd', 'cbss'], help='Algorithm to use for decomposition')

    args = parser.parse_args()
    pipelinename = args.algorithm

    root = args.bids_root

    parent_folder = root + '/Benchmarks/'

    folders = [f for f in os.listdir(parent_folder)
           if os.path.isdir(os.path.join(parent_folder, f))]

    # Only select folders generated by your pipeline 
    folders = [f for f in folders if pipelinename in f]

    datatype = 'emg'

    global_report = pd.DataFrame()
    source_report = pd.DataFrame()

    for i in np.arange(len(folders)):
        # Get the datasetname and the pipelinename
        datasetname = folders[i].split('-')[0]

        # List all files in the derivative dataset
        files = list_files(Path(parent_folder + folders[i]), '_predictedsources.edf')
        filenames = [f.name for f in files]

        for j in np.arange(len(files)):

            # Extract the relevant information of one recording
            sub, ses, task, run, _ = get_recording_info(filenames[j])

            # Get the raw data
            my_emg_data = bids_emg_recording(root=root + '/Datasets/', 
                                             datasetname=datasetname, 
                                             subject=sub, 
                                             task=task, 
                                             session=ses, 
                                             run=run, 
                                             datatype=datatype)
            
            my_emg_data.read()

            channel_idx = np.asarray(my_emg_data.channels[my_emg_data.channels['type'] == 'EMG'].index).astype(int)

            emg_data = edf_to_numpy(my_emg_data.emg_data, channel_idx)

            # Extract some metadata
            fsamp = float(my_emg_data.channels.loc[0, 'sampling_frequency'])
            target_muscle = my_emg_data.channels.loc[0, 'target_muscle']
            if datasetname == 'Caillet_et_al_2023':
                target_muscle = 'Tibialis Anterior'            

            # Get the decomposition
            my_derivative = bids_decomp_derivatives(pipelinename=pipelinename, 
                                                    root=root + '/Benchmarks/', 
                                                    datasetname=datasetname, 
                                                    subject=sub, 
                                                    task=task, 
                                                    session=ses, 
                                                    run=run, 
                                                    datatype=datatype)
            
            my_derivative.read()

            # Get global report
            my_global_report = get_global_metrics(emg_data.T, my_derivative.spikes, fsamp, my_derivative.pipeline_sidecar,
                                               filenames[j], datasetname, target_muscle=target_muscle)
            
            #outputname = (my_derivative.root + my_derivative.datapath +
            #        filenames[j].split('_predictedsources.edf')[0] + '_global_report.tsv')
            #global_report.to_csv(outputname, sep='\t', index=False, header=True)

            # Summarize all sources
            sources = edf_to_numpy(my_derivative.source,np.arange(my_derivative.source.num_signals))
            my_source_report = summarize_signal_based_metrics(sources.T, my_derivative.spikes, 
                                                              fsamp=fsamp,
                                                              datasetname=datasetname,
                                                              filename=filenames[j],
                                                              target_muscle=target_muscle
                                                              )

        
            #outputname = (my_derivative.root + my_derivative.datapath +
            #        filenames[j].split('_predictedsources.edf')[0] + '_source_report.tsv')
            #source_report.to_csv(outputname, sep='\t', index=False, header=True)

            # If availible get get ground truth / reference decomposition
            if datasetname == 'Grison_et_al_2025':
                my_ref_derivative = bids_decomp_derivatives(pipelinename='reference', 
                                                            root=root + '/Benchmarks/', 
                                                            datasetname=datasetname, 
                                                            subject=sub, 
                                                            task=task, 
                                                            session=ses, 
                                                            run=run, 
                                                            datatype=datatype)
                
                my_ref_derivative.read()
                
                start_time = my_derivative.pipeline_sidecar['AlgorithmConfiguration']['Config']['start_time']
                end_time = my_derivative.pipeline_sidecar['AlgorithmConfiguration']['Config']['end_time']
                
                df = evaluate_spike_matches(my_derivative.spikes, my_ref_derivative.spikes, 
                                            t_start = start_time, 
                                            t_end = end_time)
                
                my_source_report = pd.merge(my_source_report, df, on='unit_id')

            global_report = pd.concat([global_report, my_global_report], ignore_index=True)
            source_report = pd.concat([source_report, my_source_report], ignore_index=True)
            print('bla')
        
    global_report.to_csv(parent_folder + '_globals_' + pipelinename + '.tsv', sep='\t', index=False, header=True)
    source_report.to_csv(parent_folder + '_sources_' + pipelinename + '.tsv', sep='\t', index=False, header=True)

if __name__ == '__main__':
    main()        

